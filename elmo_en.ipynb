{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://hironsan.hatenablog.com/entry/named-entity-recognition-with-elmo\n",
    "# を実装してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMoが何かの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [['First', 'sentence', '.'], ['Another', '.']]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "embeddings = elmo(character_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1474, -0.1475,  0.1376,  ...,  0.0270, -0.4051, -0.0498],\n",
       "         [ 0.2394,  0.0769,  0.4126,  ..., -0.1671, -0.1707,  0.3884],\n",
       "         [-0.7602, -0.4944, -0.5355,  ..., -0.0803,  0.0361,  0.1128]],\n",
       "\n",
       "        [[ 0.2603, -0.4437,  0.2726,  ..., -0.0830, -0.1522, -0.1361],\n",
       "         [-0.7772, -0.4294, -0.2651,  ..., -0.0803,  0.0361,  0.1128],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<DropoutBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[\"elmo_representations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[\"elmo_representations\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELmoの分散表現を計算してみる\n",
    "与えた文について単語ごとにELMoの分散表現を得る事を目的とした例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3.08154583e-01  2.66303927e-01  2.35613093e-01 ... -3.70857447e-01\n",
      "    1.64904833e-01 -7.24597126e-02]\n",
      "  [ 5.14287531e-01 -1.35323644e-01  1.10904068e-01 ...  4.04682383e-02\n",
      "   -4.78974357e-02  7.36596107e-01]\n",
      "  [-2.58803926e-02 -7.28363097e-02 -7.93559477e-02 ... -2.90724397e-01\n",
      "    7.24214137e-01  4.38634515e-01]\n",
      "  [-3.47980678e-01 -2.91022509e-02 -8.19930434e-01 ... -9.20484006e-01\n",
      "    2.18879543e-02  1.21059902e-01]\n",
      "  [-2.18274236e-01 -1.30765587e-01 -2.52096236e-01 ... -2.96935618e-01\n",
      "   -1.58280328e-01 -4.90075573e-02]\n",
      "  [ 1.00726105e-01 -2.95349322e-02 -2.44943231e-01 ... -3.72351050e-01\n",
      "   -1.48757815e-01  2.15922311e-01]]\n",
      "\n",
      " [[ 5.45786619e-02 -2.64275432e-01  4.68437791e-01 ... -1.40770972e-01\n",
      "   -2.65682340e-01  4.52119648e-01]\n",
      "  [ 8.09428394e-02  1.15838401e-01 -1.56705871e-01 ... -2.68961281e-01\n",
      "    3.38719338e-01  1.15770502e-02]\n",
      "  [-7.89646134e-02  9.49275017e-01 -6.18049502e-01 ... -6.30558968e-01\n",
      "    3.09430093e-01  1.53787762e-01]\n",
      "  [-6.78093284e-02  9.71869081e-02 -3.62254769e-01 ...  7.75002986e-02\n",
      "   -6.16607890e-02  6.00474477e-02]\n",
      "  [-2.61795940e-04 -1.26098275e-01 -5.33946514e-01 ...  4.29217875e-01\n",
      "    1.28046393e-01  5.31919360e-01]\n",
      "  [-2.84083858e-02 -4.35321592e-02  4.13016304e-02 ...  2.58316826e-02\n",
      "   -1.42983627e-02 -1.65042114e-02]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True) # モジュール用意\n",
    "    embeddings = elmo(\n",
    "        [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
    "        signature=\"default\",\n",
    "        as_dict=True)[\"elmo\"] # モジュール適用\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        print(sess.run(embeddings)) # 実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMoを他のモデルに差し込んでみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(filename):\n",
    "    sents, labels = [], []\n",
    "    words, tags = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                word, tag = line.split('\\t')\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                sents.append(words)\n",
    "                labels.append(tags)\n",
    "                words, tags = [], []\n",
    "                \n",
    "    return sents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/conll2003/en/ner/train.txt'\n",
    "valid_file = 'data/conll2003/en/ner/valid.txt'\n",
    "\n",
    "x_train, y_train = load_data_and_labels(train_file)\n",
    "x_valid, y_valid = load_data_and_labels(valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "vocab_word = {PAD: 0, UNK: 1}\n",
    "vocab_char = {PAD: 0, UNK: 1}\n",
    "vocab_label = {PAD: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sent in x_train:\n",
    "    for w in sent:\n",
    "        # create char dictionary.\n",
    "        for c in w:\n",
    "            if c in vocab_char:\n",
    "                continue\n",
    "            vocab_char[c] = len(vocab_char)\n",
    "\n",
    "        # create word dictionary.\n",
    "        if w in vocab_word:\n",
    "            continue\n",
    "        vocab_word[w] = len(vocab_word)\n",
    "\n",
    "# create label dictionary.\n",
    "for labels in y_train:\n",
    "    for tag in labels:\n",
    "        if tag in vocab_label:\n",
    "            continue\n",
    "        vocab_label[tag] = len(vocab_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_char_sequences(x):\n",
    "    chars = []\n",
    "    for sent in x:\n",
    "        chars.append([list(w) for w in sent])\n",
    "\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_chars = get_char_sequences(x_train)\n",
    "x_valid_chars = get_char_sequences(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['E', 'U'],\n",
       " ['r', 'e', 'j', 'e', 'c', 't', 's'],\n",
       " ['G', 'e', 'r', 'm', 'a', 'n'],\n",
       " ['c', 'a', 'l', 'l'],\n",
       " ['t', 'o'],\n",
       " ['b', 'o', 'y', 'c', 'o', 't', 't'],\n",
       " ['B', 'r', 'i', 't', 'i', 's', 'h'],\n",
       " ['l', 'a', 'm', 'b'],\n",
       " ['.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_word(x):\n",
    "    seq = []\n",
    "    for sent in x:\n",
    "        word_ids = [vocab_word.get(w, vocab_word[UNK]) for w in sent]\n",
    "        seq.append(word_ids)\n",
    "\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_words = transform_word(x_train)\n",
    "x_valid_words = transform_word(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_char(x):\n",
    "    seq = []\n",
    "    for sent in x:\n",
    "        char_seq = []\n",
    "        for w in sent:\n",
    "            char_ids = [vocab_char.get(c, vocab_char[UNK]) for c in w]\n",
    "            char_seq.append(char_ids)\n",
    "        seq.append(char_seq)\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_chars = transform_char(x_train)\n",
    "x_valid_chars = transform_char(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3],\n",
       " [4, 5, 6, 5, 7, 8, 9],\n",
       " [10, 5, 4, 11, 12, 13],\n",
       " [7, 12, 14, 14],\n",
       " [8, 15],\n",
       " [16, 15, 17, 7, 15, 8, 8],\n",
       " [18, 4, 19, 8, 19, 9, 20],\n",
       " [14, 12, 11, 16],\n",
       " [21]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_label(y):\n",
    "    seq = []\n",
    "    for labels in y:\n",
    "        tag_ids = [vocab_label[tag] for tag in labels]\n",
    "        seq.append(tag_ids)\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = transform_label(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "def pad_char(sequences):\n",
    "    maxlen_word = max(len(max(seq, key=len)) for seq in sequences)\n",
    "    maxlen_seq = len(max(sequences, key=len))\n",
    "    sequences = [list(seq) + [[] for i in range(max(maxlen_seq - len(seq), 0))] for seq in sequences]\n",
    "\n",
    "    return np.array([pad_sequences(seq, padding='post', maxlen=maxlen_word) for seq in sequences])\n",
    "\n",
    "x_train_words = pad_sequences(x_train_words, padding='post')\n",
    "x_valid_words = pad_sequences(x_valid_words, padding='post')\n",
    "x_train_chars = pad_char(x_train_chars)\n",
    "x_valid_chars = pad_char(x_valid_chars)\n",
    "y_train = pad_sequences(y_train, padding='post')\n",
    "y_train = to_categorical(y_train, len(vocab_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_vocab_size = len(vocab_char)\n",
    "char_emb_size = 50\n",
    "char_lstm_units = 25\n",
    "word_vocab_size = len(vocab_word)\n",
    "word_emb_size = 100\n",
    "word_lstm_units = 100\n",
    "num_tags = len(vocab_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "word_ids = Input(shape=(None,), dtype='int32')\n",
    "char_ids = Input(shape=(None, None), dtype='int32')\n",
    "elmo_embeddings = Input(shape=(None, 1024), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import LSTM, Embedding, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "char_embeddings = Embedding(input_dim=char_vocab_size,\n",
    "                            output_dim=char_emb_size,\n",
    "                            mask_zero=True\n",
    "                            )(char_ids)\n",
    "char_embeddings = TimeDistributed(Bidirectional(LSTM(char_lstm_units)))(char_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = Embedding(input_dim=word_vocab_size,\n",
    "                            output_dim=word_emb_size,\n",
    "                            mask_zero=True)(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Concatenate(axis=-1)([word_embeddings, char_embeddings, elmo_embeddings])\n",
    "x = Bidirectional(LSTM(units=word_lstm_units, return_sequences=True))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from anago.layers import CRF\n",
    "\n",
    "x = Dense(word_lstm_units, activation='tanh')(x)\n",
    "x = Dense(num_tags)(x)\n",
    "crf = CRF(3, sparse_target=True)\n",
    "pred = crf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "model = Model(inputs=[word_ids, char_ids, elmo_embeddings], outputs=[pred])\n",
    "model.compile(loss=crf.loss_function, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When feeding symbolic tensors to a model, we expect thetensors to have a static batch size. Got tensor with shape: (None, None, 1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-be5ece904ccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melmo_embeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;34m'When feeding symbolic tensors to a model, we expect the'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;34m'tensors to have a static batch size. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 'Got tensor with shape: %s' % str(shape))\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: When feeding symbolic tensors to a model, we expect thetensors to have a static batch size. Got tensor with shape: (None, None, 1024)"
     ]
    }
   ],
   "source": [
    "model.fit([x_train_words, x_train_chars, elmo_embeddings], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
